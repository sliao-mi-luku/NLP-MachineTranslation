{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English-to-French Translator\n",
    "\n",
    "In this notebook I used the small english/french parallel text dataset prepared by Udacity in its NLP Nanodegree Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 - Import libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tensorflow.__version__)\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import sparse_categorical_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if GPU is working\n",
    "\n",
    "Please check this [link](https://medium.com/@kegui/how-do-i-know-i-am-running-keras-model-on-gpu-a9cdcc24f986) for more detail on verifying your gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 66245773583047\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3135687886\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 7853030068589879230\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Load data and preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smaller English-French dataset from Udacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data_eng_fra'\n",
    "\n",
    "# Load English data\n",
    "with open(os.path.join(data_dir, 'small_vocab_en'), 'r') as f:\n",
    "    eng_raw_data = f.read()\n",
    "          \n",
    "english_sentences = eng_raw_data.split('\\n')\n",
    "\n",
    "# Load French data\n",
    "with open(os.path.join(data_dir, 'small_vocab_fr'), 'r') as f:\n",
    "    fr_raw_data = f.read()\n",
    "          \n",
    "french_sentences = fr_raw_data.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentences of the English corpus: 137861\n",
      "sentences of the French corpus: 137861\n",
      "first English/French sentence:\n",
      "\n",
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n"
     ]
    }
   ],
   "source": [
    "## Display some info\n",
    "\n",
    "print(\"sentences of the English corpus: {}\".format(len(english_sentences)))\n",
    "print(\"sentences of the French corpus: {}\".format(len(french_sentences)))\n",
    "\n",
    "print(\"first English/French sentence:\\n\")\n",
    "print(english_sentences[0])\n",
    "print(french_sentences[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis - size of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size of English: 227\n",
      "10 most frequent English vocab: ['is', ',', '.', 'in', 'it', 'during', 'the', 'but', 'and', 'sometimes']\n",
      "Corpus size of English dataset: 1823250\n",
      "\n",
      "Vocab size of French: 355\n",
      "10 most frequent French vocab: ['est', '.', ',', 'en', 'il', 'les', 'mais', 'et', 'la', 'parfois']\n",
      "Corpus size of French dataset: 1961295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "## Size of Vob of English texts\n",
    "english_corpus = [w for sentence in english_sentences for w in sentence.split()]\n",
    "english_vocab_counter = Counter(english_corpus)\n",
    "english_vocab_size = len(english_vocab_counter)\n",
    "print(\"Vocab size of English: {}\".format(english_vocab_size))\n",
    "print(\"10 most frequent English vocab: {}\".format([x[0] for x in english_vob_counter.most_common(10)]))\n",
    "print(\"Corpus size of English dataset: {}\\n\".format(len(english_corpus)))\n",
    "\n",
    "## Size of Vob of French texts\n",
    "french_corpus = [w for sentence in french_sentences for w in sentence.split()]\n",
    "french_vocab_counter = Counter(french_corpus)\n",
    "french_vocab_size = len(french_vocab_counter)\n",
    "print(\"Vocab size of French: {}\".format(french_vocab_size))\n",
    "print(\"10 most frequent French vocab: {}\".format([x[0] for x in french_vob_counter.most_common(10)]))\n",
    "print(\"Corpus size of French dataset: {}\\n\".format(len(french_corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenizer\n",
    "def tokenize(sentences):\n",
    "    \"\"\"\n",
    "    Tokenize a list of sentences.\n",
    "    \n",
    "    input:\n",
    "        sentence: a list of sentences. ex: ['hello world', 'this is a small world']\n",
    "    output:\n",
    "        (tokenized_sentences, tokenizer): a tuple, where\n",
    "        \n",
    "        tokenized_sentences: tokenized sentence. ex: [[1 , 2], [3, 4, 5, 6, 2]]\n",
    "        tokenizer: the tokenizer\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    tokenized_sentences = tokenizer.texts_to_sequences(sentences)\n",
    "    \n",
    "    return (tokenized_sentences, tokenizer)\n",
    "\n",
    "\n",
    "## Padding\n",
    "\n",
    "def pad(sentences, pad_length = None):\n",
    "    \"\"\"\n",
    "    Pad every sentence in sentences to the length of pad_length\n",
    "    If pad_length is None, it is set as the maximum length among the sentences\n",
    "    \n",
    "    inputs:\n",
    "        sentences: a list of sentences. ex: [[1 , 2], [3, 4, 5, 6, 2]]\n",
    "        pad_length: final length after padding\n",
    "    \n",
    "    output:\n",
    "        padded_sentences: padded sentences. ex: [[1, 2, 0, 0, 0], [3, 4, 5, 6, 2]]\n",
    "    \"\"\"\n",
    "    padded_sentences = pad_sequences(sentences, maxlen = pad_length, padding = 'post')\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to convert ouput into readable sentences\n",
    "\n",
    "The output will be transformed back to human-readable sentence by referring to the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_to_texts(softmax_result, tokenizer):\n",
    "    \"\"\"\n",
    "    Transform output into the text sentence\n",
    "    \n",
    "    inputs:\n",
    "        softmax_result: the output from neural networks. shape = (padded_length, vocab_size)\n",
    "        tokenizer: the tokenizer used to translate\n",
    "    \"\"\"\n",
    "    # reverse tokenizer.word_index dict\n",
    "    decoding_dict = {token: vocab for vocab, token in tokenizer.word_index.items()}\n",
    "    # define 0 to be None\n",
    "    decoding_dict[0] = ''\n",
    "    # decode into a sentence\n",
    "    sentence = ' '.join([decoding_dict[token] for token in np.argmax(softmax_result, 1)])\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation demo function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of input (English sentences): (137861, 15)\n",
      "maximum length of the English sentence: 15\n",
      "shape of output (French sentences): (137861, 21, 1)\n",
      "maximum length of the French sentence: 21\n"
     ]
    }
   ],
   "source": [
    "## Process English and French data\n",
    "\n",
    "eng_tokenized_sentences, eng_tokenizer = tokenize(english_sentences)\n",
    "eng_padded_sentences = pad(eng_tokenized_sentences, pad_length = None)\n",
    "print(\"shape of input (English sentences): {}\".format(eng_padded_sentences.shape))\n",
    "del eng_tokenized_sentences\n",
    "\n",
    "eng_sentence_maxlen = eng_padded_sentences.shape[1]\n",
    "print(\"maximum length of the English sentence: {}\".format(eng_sentence_maxlen))\n",
    "\n",
    "\n",
    "\n",
    "fr_tokenized_sentences, fr_tokenizer = tokenize(french_sentences)\n",
    "fr_padded_sentences = pad(fr_tokenized_sentences, pad_length = None)\n",
    "fr_padded_sentences = fr_padded_sentences.reshape(*fr_padded_sentences.shape, 1)\n",
    "print(\"shape of output (French sentences): {}\".format(fr_padded_sentences.shape))\n",
    "del fr_tokenized_sentences\n",
    "\n",
    "fr_sentence_maxlen = fr_padded_sentences.shape[1]\n",
    "print(\"maximum length of the French sentence: {}\".format(fr_sentence_maxlen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction function\n",
    "\n",
    "def predict_demo(model, test_indeces):\n",
    "    \"\"\"\n",
    "    Print out several sentences and see their translations\n",
    "    \n",
    "    inputs\n",
    "            model: model used for prediction\n",
    "            test_indeces: a list of indeces of the test set to evaluate the model\n",
    "    \"\"\"\n",
    "\n",
    "    for test_index in test_indeces:\n",
    "        \n",
    "        eng_sentence = english_sentences[test_index]\n",
    "        answer_sentence = french_sentences[test_index]\n",
    "        prediction_logits = model.predict(eng_padded_sentences[test_index].reshape(-1, eng_sentence_maxlen))[0]\n",
    "        prediction_sentence = output_to_texts(prediction_logits, fr_tokenizer)\n",
    "        \n",
    "        \n",
    "        print(eng_sentence)\n",
    "        print(answer_sentence)\n",
    "        print(prediction_sentence)\n",
    "        print(\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Benchmark model (deep RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark model\n",
    "\n",
    "The benchmark model uses bidirectional GRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_benchmark_model(input_shape,\n",
    "                        embedding_size,\n",
    "                        GRU_units,\n",
    "                        output_sentence_length,\n",
    "                        input_vocab_size,\n",
    "                        output_vocab_size,\n",
    "                        lr):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "            input_shape\n",
    "            output_sentence_length\n",
    "            input_vocab_size\n",
    "            output_vocab_size\n",
    "    \"\"\"\n",
    "    # input\n",
    "    inputs = Input(shape = input_shape[1:]) # remove the dimension of the batch\n",
    "    # deep RNN\n",
    "    x = Embedding(input_dim = input_vocab_size, output_dim = embedding_size)(inputs)\n",
    "    x = Bidirectional(GRU(GRU_units))(x)\n",
    "    x = RepeatVector(output_sentence_length)(x)\n",
    "    x = Bidirectional(GRU(GRU_units, return_sequences = True))(x)\n",
    "    x = TimeDistributed(Dense(output_vocab_size))(x)\n",
    "    x = Activation('softmax')(x)\n",
    "    # build model\n",
    "    model = Model(inputs = inputs, outputs = x)\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(loss = sparse_categorical_crossentropy,\n",
    "                  optimizer = Adam(lr),\n",
    "                  metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 15)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 15, 256)           58112     \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 256)               296448    \n",
      "_________________________________________________________________\n",
      "repeat_vector_4 (RepeatVecto (None, 21, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 21, 256)           296448    \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 21, 355)           91235     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 21, 355)           0         \n",
      "=================================================================\n",
      "Total params: 742,243\n",
      "Trainable params: 742,243\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/20\n",
      "110288/110288 [==============================] - 21s 190us/sample - loss: 2.8587 - accuracy: 0.4418 - val_loss: 2.0253 - val_accuracy: 0.5052\n",
      "Epoch 2/20\n",
      "110288/110288 [==============================] - 14s 127us/sample - loss: 1.8163 - accuracy: 0.5450 - val_loss: 1.6255 - val_accuracy: 0.5823\n",
      "Epoch 3/20\n",
      "110288/110288 [==============================] - 14s 128us/sample - loss: 1.4480 - accuracy: 0.6245 - val_loss: 1.3312 - val_accuracy: 0.6530\n",
      "Epoch 4/20\n",
      "110288/110288 [==============================] - 14s 127us/sample - loss: 1.2229 - accuracy: 0.6751 - val_loss: 1.1420 - val_accuracy: 0.6920\n",
      "Epoch 5/20\n",
      "110288/110288 [==============================] - 14s 131us/sample - loss: 1.0877 - accuracy: 0.7032 - val_loss: 1.0395 - val_accuracy: 0.7141\n",
      "Epoch 6/20\n",
      "110288/110288 [==============================] - 14s 131us/sample - loss: 0.9920 - accuracy: 0.7235 - val_loss: 0.9516 - val_accuracy: 0.7310\n",
      "Epoch 7/20\n",
      "110288/110288 [==============================] - 14s 126us/sample - loss: 0.9220 - accuracy: 0.7356 - val_loss: 0.8844 - val_accuracy: 0.7425\n",
      "Epoch 8/20\n",
      "110288/110288 [==============================] - 14s 128us/sample - loss: 0.8495 - accuracy: 0.7526 - val_loss: 0.8139 - val_accuracy: 0.7617\n",
      "Epoch 9/20\n",
      "110288/110288 [==============================] - 15s 136us/sample - loss: 0.7792 - accuracy: 0.7737 - val_loss: 0.7465 - val_accuracy: 0.7831\n",
      "Epoch 10/20\n",
      "110288/110288 [==============================] - 15s 140us/sample - loss: 0.7195 - accuracy: 0.7897 - val_loss: 0.6943 - val_accuracy: 0.7952\n",
      "Epoch 11/20\n",
      "110288/110288 [==============================] - 16s 141us/sample - loss: 0.6598 - accuracy: 0.8063 - val_loss: 0.6325 - val_accuracy: 0.8128\n",
      "Epoch 12/20\n",
      "110288/110288 [==============================] - 16s 141us/sample - loss: 0.6039 - accuracy: 0.8227 - val_loss: 0.5827 - val_accuracy: 0.8277\n",
      "Epoch 13/20\n",
      "110288/110288 [==============================] - 16s 141us/sample - loss: 0.5575 - accuracy: 0.8365 - val_loss: 0.5326 - val_accuracy: 0.8432\n",
      "Epoch 14/20\n",
      "110288/110288 [==============================] - 16s 141us/sample - loss: 0.5047 - accuracy: 0.8529 - val_loss: 0.4870 - val_accuracy: 0.8580\n",
      "Epoch 15/20\n",
      "110288/110288 [==============================] - 16s 141us/sample - loss: 0.4576 - accuracy: 0.8685 - val_loss: 0.4404 - val_accuracy: 0.8744\n",
      "Epoch 16/20\n",
      "110288/110288 [==============================] - 16s 141us/sample - loss: 0.4131 - accuracy: 0.8846 - val_loss: 0.3927 - val_accuracy: 0.8917\n",
      "Epoch 17/20\n",
      "110288/110288 [==============================] - 16s 144us/sample - loss: 0.3703 - accuracy: 0.8984 - val_loss: 0.3507 - val_accuracy: 0.9042\n",
      "Epoch 18/20\n",
      "110288/110288 [==============================] - 16s 144us/sample - loss: 0.3306 - accuracy: 0.9095 - val_loss: 0.3245 - val_accuracy: 0.9108\n",
      "Epoch 19/20\n",
      "110288/110288 [==============================] - 16s 143us/sample - loss: 0.2941 - accuracy: 0.9200 - val_loss: 0.2850 - val_accuracy: 0.9231\n",
      "Epoch 20/20\n",
      "110288/110288 [==============================] - 16s 144us/sample - loss: 0.2660 - accuracy: 0.9286 - val_loss: 0.2534 - val_accuracy: 0.9326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1bbc5e81e08>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## build benchmark model\n",
    "\n",
    "benchmark_model = build_benchmark_model(input_shape = (eng_padded_sentences.shape[0], eng_sentence_maxlen),\n",
    "                        embedding_size = 256,\n",
    "                        GRU_units = 128,\n",
    "                        output_sentence_length = fr_sentence_maxlen,\n",
    "                        input_vocab_size = english_vocab_size,\n",
    "                        output_vocab_size = french_vocab_size,\n",
    "                        lr = 1e-3)\n",
    "\n",
    "benchmark_model.summary()\n",
    "\n",
    "## train\n",
    "benchmark_model.fit(eng_padded_sentences, fr_padded_sentences, batch_size = 1024, epochs = 20, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "they like strawberries , pears , and bananas .\n",
      "ils aiment les fraises , les poires et les bananes .\n",
      "ils aiment les fraises les poires et les bananes            \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Predictions\n",
    "predict_demo(benchmark_model, [1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Attention models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
